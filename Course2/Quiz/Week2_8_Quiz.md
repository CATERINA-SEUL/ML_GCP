#### 1. Which statement is true regarding the “dropout technique” used in neural networks?

A : Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.

#### 2. Which of the following statements is true?

A : Dropout can help a model generalize by randomly setting the output for a given neuron to 0. In setting the output to 0, the cost function becomes more sensitive to neighbouring neurons changing the way the weights will be updated during the process of backpropagation.

#### 3. Which statement is true regarding neural networks?

    Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.

A : All of the above
    
    Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns.
    Neural networks interpret sensory data through a kind of machine perception, labeling or clustering raw input. 
    The patterns neural networks recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.

#### 4. Which of the following is not a type of modern neural network?

A : Sine Neural Network

#### 5. Which of the following are ways to improve generalization?

A : All of the above
    
    Adding dropout layers. Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.
    Performing data augmentation, which is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.
    Adding noise - for example, adding Gaussian noise to input variables.
            Gaussian noise, or white noise, has a mean of zero and a standard deviation of one and can be generated as needed using a pseudorandom number generator. 

