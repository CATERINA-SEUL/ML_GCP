#### 1. Select the correct statement(s) regarding gradient descent.

A : Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. 

#### 2. Select which statement is true.

A : Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it's called a training epoch.

#### 3. Fill in the blanks. In the ________________________ method, one training sample (example) is passed through the neural network at a time and the parameters (weights) of each layer are updated with the computed gradient. 

A : Stochastic Gradient Descent

#### 4. Which of the following gradient descent methods is used to compute the entire dataset?

A : Batch gradient descent 

#### 5. Fill in the blanks. 

    ________________: Parameters are updated after computing the gradient of error with respect to the entire training set
    ________________: Parameters are updated after computing the gradient of error with respect to a single training example
    ________________: Parameters are updated after computing the gradient of error with respect to a subset of the training set

A : Batch Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent