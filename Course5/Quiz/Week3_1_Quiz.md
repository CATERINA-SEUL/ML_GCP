#### 1. Which of the following statements are correct about Batch Normalization?

A : All of the above

    Solves the problem called internal co-variance shift
    Helps with exploding gradients
    Helps the intermediate inputs at each layer stay within a tighter range

#### 2. Which of the following gives non-linearity to a neural network?

A : Rectified Linear Unit

#### 3. During the training process for deep networks, gradients can vanish, and each additional layer in your network can successively reduce signal vs noise How can you fix this?

A : Use non-saturating, nonlinear activation functions such as ReLUs.

#### 4. How can you solve the problem where gradients explode?

A : Both A and B

    Grading and clipping
    Batch normalization

#### 5. In a neural network, which of the following techniques is used to deal with overfitting?

A : All of these

    Dropout
    Regularization
    Batch Normalization