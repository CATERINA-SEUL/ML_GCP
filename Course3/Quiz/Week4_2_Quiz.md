#### 1. The activations in regularization is scaled by which of the following ?

A : 1 / (1 - dropout probability)

#### 2. How does regularization help build generalizable models ?

A : By adding dropout layers to our neural networks.

#### 3. The L2 regularization provides which of the following?

A : It adds a sum of the squared parameter weights term to the loss function.

#### 4. Which of the following statements is true?

A : L2 regularization will keep the weight values smaller and L1 regularization will make the model sparser by dropping poor features.

#### 5. Which is an approximate equivalent of L2 regularization?

A : Early Stopping