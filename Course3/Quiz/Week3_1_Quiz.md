#### 1. Non-linearity helps in training your model at a much faster rate and with more accuracy without the loss of your important information?

A : True

#### 2. The activation function which is linear in the positive domain and the function is 0 in the negative domain?

A : ReLU

#### 3. During the training process, each additional layer in your network can successively reduce signal vs. noise.  How can we fix this?

A : Use non-saturating, nonlinear activation functions such as ReLUs.

#### 4. How can we solve the problem called internal covariate shift?

A : Batch normalization

#### 5. How can we stop ReLU layers from dying?

A : Lower your learning rates
